# -*- coding: utf-8 -*-
"""Medical_Insurance_cost .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zh3tPrg5aJRcUDmBf6k0qUR07tOAukLV
"""

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
path='/content/drive/MyDrive/Dataset/Train_Data.csv'
df=pd.read_csv(path)
df.head()

# Select numeric columns (e.g., int64, float64)
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Round numeric columns to whole numbers
df[numeric_columns] = df[numeric_columns].round(0).astype(int)

# Preview the updated DataFrame
print(df.head())

def column_summary(df):
    summary_data = []

    for col_name in df.columns:
        col_dtype = df[col_name].dtype
        num_of_nulls = df[col_name].isnull().sum()
        num_of_non_nulls = df[col_name].notnull().sum()
        num_of_distinct_values = df[col_name].nunique()

        if num_of_distinct_values <= 10:
            distinct_values_counts = df[col_name].value_counts().to_dict()
        else:
            top_10_values_counts = df[col_name].value_counts().head(10).to_dict()
            distinct_values_counts = {k: v for k, v in sorted(top_10_values_counts.items(), key=lambda item: item[1], reverse=True)}

        summary_data.append({
            'col_name': col_name,
            'col_dtype': col_dtype,
            'num_of_nulls': num_of_nulls,
            'num_of_non_nulls': num_of_non_nulls,
            'num_of_distinct_values': num_of_distinct_values,
            'distinct_values_counts': distinct_values_counts
        })

    summary_df = pd.DataFrame(summary_data)
    return summary_df

summary_df = column_summary(df)
display(summary_df)

# Shape and data types
print("\nDataset shape:", df.shape)
print("\nData types:\n", df.dtypes)

# Summary statistics
print("\nSummary statistics:\n", df.describe(include='all'))

# Null values
print("\nMissing values:\n", df.isnull().sum())

# @title Distribution
import seaborn as sns
import matplotlib.pyplot as plt

# Histograms
df.hist(figsize=(12, 8))
plt.tight_layout()
plt.show()

# @title Smoker

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('smoker').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title Sex

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('sex').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title Region

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('region').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title Correlation Heatmap
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# @title Age vs Medical Charges by Smoking Status
sns.scatterplot(data=df, x='age', y='charges', hue='smoker')
plt.title("Age vs Medical Charges by Smoking Status")
plt.show()

# @title Medical Charges by Smoking Status
sns.boxplot(x='smoker', y='charges', data=df)
plt.title("Medical Charges by Smoking Status")
plt.show()

# @title BMI vs Charges, Separated by Smoking Status
sns.scatterplot(x='bmi', y='charges', hue='smoker', data=df)
plt.title("BMI vs Charges, Separated by Smoking Status")
plt.show()

# @title Charges by Region
sns.boxplot(x='region', y='charges', data=df)
plt.title("Charges by Region")
plt.xticks(rotation=45)
plt.show()

# @title Distribution of Medical Charges
plt.figure(figsize=(8, 5))
sns.histplot(df['charges'], kde=True)
plt.title("Distribution of Medical Charges")
plt.show()

# @title Boxplot of Medical Charges
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
sns.boxplot(x=df['charges'])
plt.title("Boxplot of Medical Charges")
plt.show()

# Using log transfor to make the distribution more normal
import numpy as np

df['charges_log'] = np.log1p(df['charges'])  # log1p = log(1 + x) to avoid log(0)
df

# @title Boxplot of Log-Transformed Charges
sns.boxplot(x=df['charges_log'])
plt.title("Boxplot of Log-Transformed Charges")
plt.show()

# Categorical variables
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

print("Categorical variables:")
print(categorical_cols)

# Numeric variables
numeric_cols = df.select_dtypes(include=['int64', 'float64','int32','float32']).columns.tolist()

print("numeric variables:")
print(numeric_cols)

from scipy.stats import zscore

def remove_outliers(df, columns=None, method='iqr', threshold=3, contamination=0.05):

    df_clean = df.copy()

    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns.tolist()

    # Method 1: Z-score (for normal distributions)
    if method == 'zscore':
        z_scores = df[columns].apply(zscore)
        df_clean = df[(np.abs(z_scores) < threshold).all(axis=1)]

    # Method 2: IQR (for skewed data)
    elif method == 'iqr':
        mask = pd.Series(True, index=df.index)
        for col in columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            mask &= (df[col] >= Q1 - threshold*IQR) & (df[col] <= Q3 + threshold*IQR)
        df_clean = df[mask]

    print(f"Removed {len(df) - len(df_clean)} outliers ({method} method)")
    return df_clean
df_clean = remove_outliers(df, columns=numeric_cols, method='iqr', threshold=1.5)

df_clean.sample(5)

print(f"Original: {df.shape[0]} rows")
print(f"After removing outliers: {df_clean.shape[0]} rows")

# Use transformed charges if needed
target = 'charges'  # or 'charges' if you're using raw values

X = df.drop(['charges', 'charges_log'], axis=1)
y = df[target]

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge, Lasso, LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor  # ✅ Add XGBoost

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define preprocessing steps
categorical_cols = ['sex', 'smoker', 'region']
numerical_cols = ['age', 'bmi', 'children']

preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), numerical_cols),
    ('cat', OneHotEncoder(drop='first'), categorical_cols)
])

# Define models and hyperparameters
models = {
    'LinearRegression': {
        'model': LinearRegression(),
        'params': {
            # No hyperparameters to tune
        }
    },
    'Ridge': {
        'model': Ridge(),
        'params': {
            'model__alpha': [0.1, 1.0, 10.0, 50.0]
        }
    },
    'Lasso': {
        'model': Lasso(),
        'params': {
            'model__alpha': [0.001, 0.01, 0.1, 1.0]
        }
    },
    'RandomForest': {
        'model': RandomForestRegressor(random_state=42),
        'params': {
            'model__n_estimators': [100, 200],
            'model__max_depth': [None, 10, 20]
        }
    },
    'GradientBoosting': {
        'model': GradientBoostingRegressor(random_state=42),
        'params': {
            'model__n_estimators': [100, 200],
            'model__learning_rate': [0.05, 0.1],
            'model__max_depth': [3, 5]
        }
    },
    'XGBoost': {
        'model': XGBRegressor(random_state=42, objective='reg:squarederror'),
        'params': {
            'model__n_estimators': [100, 200],
            'model__learning_rate': [0.05, 0.1],
            'model__max_depth': [3, 5]
        }
    }
}

# Run GridSearchCV for all models
best_models = {}

for name, config in models.items():
    print(f"🔍 Tuning {name}...")

    pipeline = Pipeline(steps=[
        ('preprocess', preprocessor),
        ('model', config['model'])
    ])

    grid = GridSearchCV(
        pipeline,
        config['params'],
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )

    grid.fit(X_train, y_train)
    best_models[name] = grid

    print(f"✅ Best score (MSE): {-grid.best_score_:.2f}")
    print(f"🔧 Best params: {grid.best_params_}\n")

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
for name, model in best_models.items():
    # Predict on test data
    y_pred = model.predict(X_test)

    # Calculate metrics
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    # Print results
    print(f"📈 {name} Performance on Test Set")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  R² Score: {r2:.4f}\n")

path='/content/drive/MyDrive/Dataset/Test_Data.csv'
test_df=pd.read_csv(path)
test_df

# Preview
print(test_df.columns)

# Optional: Check for missing data
print(test_df.isnull().sum())

# Use the best Random Forest model from earlier
rf_model = best_models['RandomForest'].best_estimator_

# Predict charges
test_predictions = rf_model.predict(test_df)
test_predictions

test_df['charges'] = test_predictions
test_df

import pickle

best_model = best_models['RandomForest'].best_estimator_

# Save to a .pkl file
with open("best_model.pkl", "wb") as model_file:
    pickle.dump(best_model, model_file)
with open('preprocessor.pkl', 'wb') as preprocessor_file:
    pickle.dump(preprocessor, preprocessor_file)

print("✅ Model saved as best_model.pkl")